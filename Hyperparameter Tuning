import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Step 1: Create synthetic binary classification dataset
input_dim = 20  # You can adjust this depending on the dataset
X, y = make_classification(
    n_samples=1000, n_features=input_dim, n_informative=5, n_redundant=2, 
    n_clusters_per_class=1, n_classes=2, random_state=42
)

# Step 2: Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 3: Build a model with optimized architecture and hyperparameters

model = Sequential([
    Dense(64, activation='relu', input_shape=(input_dim,), kernel_regularizer=l2(0.001)),  # L2 regularization
    Dropout(0.3),  # Dropout for regularization
    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.3),
    Dense(16, activation='relu', kernel_regularizer=l2(0.001)),
    Dense(1, activation='sigmoid')  # Sigmoid activation for binary classification
])

# Step 4: Compile the model with an optimizer and learning rate adjustment
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)  # You can try different values like 0.0001 or 0.01
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Step 5: Use EarlyStopping to stop training when validation loss is no longer improving
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Step 6: Train the model with validation split
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])

# Step 7: Evaluate the model on the test set
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")

# Step 8: Visualize training history
# Accuracy plot
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Loss plot
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
